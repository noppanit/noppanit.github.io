<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Database | My Blog]]></title>
  <link href="http://www.noppanit.com/blog/categories/database/atom.xml" rel="self"/>
  <link href="http://www.noppanit.com/"/>
  <updated>2015-10-02T10:19:31-04:00</updated>
  <id>http://www.noppanit.com/</id>
  <author>
    <name><![CDATA[Noppanit Charassinvichai]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Lesson Learnt From Data Cleansing. Part II]]></title>
    <link href="http://www.noppanit.com/lesson-learnt-from-data-cleansing-part-ii/"/>
    <updated>2013-02-11T00:00:00-05:00</updated>
    <id>http://www.noppanit.com/lesson-learnt-from-data-cleansing-part-ii</id>
    <content type="html"><![CDATA[<p>Ok as I have done the <a href="http://www.noppanit.com/lesson-learnt-from-data-cleansing/" title="data cleansing part I">part I</a> I have learnt another thing from data cleansing. I believe the best tool for data analysis is <a href="http://www.r-project.org/" title="R">R</a> but the learning is quite steep but I&#8217;m trying to learn it right now. Hopefully if I can get some results from this with neo4j I could move on to use R to get some more statistical results.</p>

<p>Now, the second challenge I got was that the CSV file is about 1GB with around 100 columns and I was trying to use Ruby to parse that. Obviously, I got <strong>Killed</strong> results from my Terminal even before I got to the first line. So, Ruby has to load everything into memory and parse that. But as I&#8217;m running everything in my small Mac. It seems to be quite a challenging. So, what I have done is to dump everything to MySQL. This time works really good. MySQL has <a href="http://dev.mysql.com/doc/refman/5.1/en/load-data.html" title="Load data mysql">LOAD DATA FILE</a> which can load CSV file into a table. It only took me around <string>20 seconds</strong> to load 1GB of CSV into a table. I dumped the table out and it was around 1GB which is not too bad. Now the next challenge was that I also had to filter out the events that were created by real users (I&#8217;m doing this <a href="http://www.kaggle.com/c/event-recommendation-engine-challenge" title="kaggle competition">Kaggle competition</a>). I had around 3 million rows which is now that much. And I used MySQL which comes with XAMPP. My first query took me around 8 hours without spitting any results back. So, I gave up and did a couple research on how to make MySQL faster, but I&#8217;m not a DBA and I didn&#8217;t have time to read the whole book about MySQL performance tuning.</p>

<p><strong>First attempt</strong><br/>
Index joined columns and changed the table schema to something small. I used <strong>VARCHAR(50)</strong> instead of <strong>VARCHAR(255)</strong> and of course I used <strong>NUMBER(3)</strong> instead of <strong>VARCHAR(5)</strong></p>

<p>Still, I couldn&#8217;t get the results back in 1 hour which I think MySQL is better than that.</p>

<p><strong>Second attempt</strong><br/>
So, I moved on to the next one. It looked like everybody on the Internet suggested <strong>key_buffer_size</strong> or <strong>key_buffer</strong> to 4G. And changed some of the variables to pimp MySQL up a little bit.</p>

<pre><code>  * query\_cache\_type = 1
  * query\_cache\_size = 128M
  * query\_cache\_limit = 128M&gt;
</code></pre>

<p>Still, I couldn&#8217;t get the results back.</p>

<p>And suddenly I caught something in my <strong>my.cnf</strong></p>

<pre><code># Example MySQL config file for small systems.
# This is for a system with little memory (&amp;lt;= 64M) where MySQL is only used
# from time to time and it's important that the mysqld daemon
# doesn't use much resources.
#
</code></pre>

<p>What is said was that XAMPP is tuned for a very small computer to be able to run basic PHP and MySQL application which is why my MySQL performed really bad in this case.</p>

<p><strong>Final attempt</strong><br/>
I started my EC2 instance with 7GB of memory and installed MySQL (normal one) also some of the performance tuning variables. Now, I got the results back in just under 20 seconds to join two tables with around 3 millions rows.</p>

<blockquote><p>Lesson learnt: Do not use MySQL from XAMPP for data that is big and use query that would get smaller results. For example, in my case instead of filtering out user_ids that don&#8217;t exist in Users table. I filtered user_ids that exist instead.</p></blockquote>

<p>I also thought about of using BigQuery from Google but I guess my data is rather small and can hardly be called big data. Next time, I might try to use BigQuery just for fun.</p>
]]></content>
  </entry>
  
</feed>

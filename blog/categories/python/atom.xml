<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Python | My Blog]]></title>
  <link href="http://noppanit.github.io/blog/categories/python/atom.xml" rel="self"/>
  <link href="http://noppanit.github.io/"/>
  <updated>2015-10-02T07:15:06-04:00</updated>
  <id>http://noppanit.github.io/</id>
  <author>
    <name><![CDATA[Noppanit Charassinvichai]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Python to Import Your Dumped Tweets to MongoDb]]></title>
    <link href="http://noppanit.github.io/python-import-dumped-tweets-mongodb/"/>
    <updated>2013-09-04T00:00:00-04:00</updated>
    <id>http://noppanit.github.io/python-import-dumped-tweets-mongodb</id>
    <content type="html"><![CDATA[<p>I&#8217;m playing around with my tweets. Just so you&#8217;re aware that you could download your entire tweets and play around with it. The format is JSON so I think it makes perfect sense to dump this to MongoDB. But you can&#8217;t just import that straight away it needs some manipulation. I&#8217;m not good at Python so the code here might be tedious for python dudes. I&#8217;m going to use this data in my analysis, which will be captured in the next blog post.</p>

<pre><code class="python">import pymongo
from pprint import pprint

path = './data'
client = pymongo.MongoClient('localhost', 27017)
db = client.tweets
def main():
        for infile in glob.glob( os.path.join(path, "*.js")):
                content = open(infile).read()
                indexOfFirstEqualSign = content.find("=") + 1
                pureJson = content[indexOfFirstEqualSign:]
                jsonifedData = json.loads(pureJson)
                db.tweets_collection.insert(jsonifedData)

if __name__ == "__main__":
        main()
</code></pre>

<p>The data is kept in <strong>data</strong> folder.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Flask and RQ Example for Heroku]]></title>
    <link href="http://noppanit.github.io/flask-and-rq-example-for-heroku/"/>
    <updated>2013-03-08T00:00:00-05:00</updated>
    <id>http://noppanit.github.io/flask-and-rq-example-for-heroku</id>
    <content type="html"><![CDATA[<p>I have been struggling to find any example on the Internet on how to do long polling on Heroku with Flask and RQ. I know it&#8217;s relatively easy, but I just want to make it clear for my future self.</p>

<p>It&#8217;s really simple.</p>

<pre><code class="python">@app.route('/get_word_count', methods=['POST'])
def get_word_count():
    data_json = json.loads(request.data)
    job = q.enqueue(word_counter.count_words, data_json["sentence"])
    return job.key

@app.route("/get_word_count_result/&lt;job_key&gt;", methods=['GET'])
def get_word_count_result(job_key):
    job_key = job_key.replace("rq:job:", "")
    job = Job.fetch(job_key, connection=conn)

    if(not job.is_finished):
        return "Not yet", 202
    else:
        return str(job.result), 200
</code></pre>

<p>The key is here</p>

<pre><code class="python">job = Job.fetch(job_key, connection=conn)
</code></pre>

<p>The fetch command is used to get the job by job_id, you can also use</p>

<pre><code class="python">get_current_job()
</code></pre>

<p>as well, but I just want to make it clear that if there are more concurrent requests coming in, I&#8217;ll get the right result back.</p>

<p>I posted an example here. <a href="https://github.com/noppanit/heroku-flask-rq-worker" title="Python with RQ on Heroku">https://github.com/noppanit/heroku-flask-rq-worker</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Second Sentiment Analysis Experiment on Naive Bayes With NLTK : Bigrams]]></title>
    <link href="http://noppanit.github.io/second-sentiment-analysis-experiment-on-naive-bayes-with-nltk-bigrams/"/>
    <updated>2012-12-30T00:00:00-05:00</updated>
    <id>http://noppanit.github.io/second-sentiment-analysis-experiment-on-naive-bayes-with-nltk-bigrams</id>
    <content type="html"><![CDATA[<p>From my <a href="http://www.noppanit.com/first-experiment-on-naive-bayes-with-nltk/" title="first experiment on naive bayes with sentiment analysis">last post</a> I experimented with some of the techniques such as stopwords and bag-of-words model. I yielded some acceptable results. This post, I&#8217;m going to try with bigrams to see if I can increase the accuracy.</p>

<p>I changed the code a little bit to be</p>

<pre><code class="python">from nltk.collocations import *

tokenized_text = nltk.wordpunct_tokenize(words)
tokenized_text = [word.lower() for word in tokenized_text]

finder = BigramCollocationFinder.from_words(tokenized_text)
bigrammed_words = sorted(finder.nbest(bigram_measures.chi_sq, 200))
</code></pre>

<p>I decided to use <strong>chi_sq</strong> as suggested in <a href="http://streamhacker.com/tag/bigrams/" title="stream hacker bigrams">this post</a>. However, the accuracy has gone down significantly to <strong>19.7530864198%</strong>. I guess this might be that my document (~100 document for each sentiment) is not large enough to use bigrams. But this is just my conclusion. I&#8217;m going to try to increase the dataset and test it again.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[First Experiment on Naive Bayes With NLTK]]></title>
    <link href="http://noppanit.github.io/first-experiment-on-naive-bayes-with-nltk/"/>
    <updated>2012-12-30T00:00:00-05:00</updated>
    <id>http://noppanit.github.io/first-experiment-on-naive-bayes-with-nltk</id>
    <content type="html"><![CDATA[<p>I have been experimenting with Natural Language Processing on Text classification for a while now. So, I&#8217;m going to write a little journal on my blog. There are lots of academic papers or event commercial API on the Internet for sentiment analysis. But most of them only classify sentiment into <strong>negative</strong>, <strong>positive</strong> and <strong>neutral</strong>. My experiment will be based on <a href="http://en.wikipedia.org/wiki/Plutchik%27s_Wheel_of_Emotions#Plutchik.27s_wheel_of_emotions" title="Plutchik wheel of emotions">Plutchik&#8217;s wheel of emotions</a> which will classify a text into one of the eight emotions.</p>

<p>For the purpose to get things done really fast, I use the example from <a href="http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/" title="twitter sentiment analysis">Laurent&#8217;s blog</a>. But you can use <a href="https://github.com/japerk/nltk-trainer" title="nltk trainer">nltk-trainer</a> to train the classifier without a single line of python code.</p>

<p>Most papers suggest that <a href="http://en.wikipedia.org/wiki/Bag-of-words_model" title="bag of words model">bag-of-words model</a> is one of the best techniques o classify text. So, I decided to use this method. However, this is about sentiment analysis so I used only Adjectives for feature extraction. The result is unacceptable with only <strong>19.7530864198 %</strong></p>

<pre><code class="python">for word, pos_tag in nltk.pos_tag(words):
   if pos_tag == 'ADJ':
      filtered_words.append(word.lower())
</code></pre>

<p>The second attempt I decided to fall back to bag-of-words model, and the result has gone up to <strong>61.316872428 %</strong></p>

<pre><code class="python">filtered_words = [e.lower() for e in words.split() if len(e) &gt;= 3]
</code></pre>

<p>So, I moved on and try to clean up the text a bit by cleaning stopwords, stripping &#8216;RT&#8217; or &#8216;rt&#8217; for retweet, deleting @peoplename and tokenise word by whitespace. So, &#8220;i&#8217;m&#8221; stays as one word and not [&#8220;i&#8221;, &#8220;&#8216;m&#8221;]. The result has gone up to <strong>69.9588477366 %</strong>.</p>

<pre><code class="python"># stripping and cleaning.
# this is for stripping out the stopwords by using &lt;a href="http://nltk.googlecode.com/svn/trunk/doc/book/ch02.html" title="NLTK Text Corpora"&gt;NLTK Text Corpora&lt;/a&gt;
stripped_words = [w for w in tokenized_text if not w in stopwords.words('english')]
</code></pre>

<p>I&#8217;ll keep experimenting and post some more techniques to see if I could get something out of this.</p>
]]></content>
  </entry>
  
</feed>

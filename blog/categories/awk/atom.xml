<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: awk, | My Blog]]></title>
  <link href="https://www.noppanit.com/blog/categories/awk/atom.xml" rel="self"/>
  <link href="https://www.noppanit.com/"/>
  <updated>2017-10-14T10:50:11-04:00</updated>
  <id>https://www.noppanit.com/</id>
  <author>
    <name><![CDATA[Noppanit Charassinvichai]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How I learn to love awk]]></title>
    <link href="https://www.noppanit.com/how-i-learn-to-love-awk/"/>
    <updated>2017-10-14T10:24:39-04:00</updated>
    <id>https://www.noppanit.com/how-i-learn-to-love-awk</id>
    <content type="html"><![CDATA[<h2 id="problem-statement">Problem statement</h2>

<p>I’m writing a Jenkins job to produce a file with a list of canonical urls from two separate sources. One source looks like this</p>

<p>``` text</p>

<p>File brand_urls</p>

<p>“url1”,”brand”
“url2”,”brand”
“url3”,”brand1”</p>

<p>```</p>

<p>The second looks like this</p>

<p>``` text</p>

<p>File url.json after manipulation using jq</p>

<p>The source looks like this</p>

<p>[{
  slug: “brand”,
  …,
  domain: {
    production: ‘www.domain.com’
  }
 }, {
  slug: “brand1”,
  …,
  domain: {
    production: ‘www.domain1.com’
}]</p>

<h1 id="echo-brandjson--jq--r---domainproductionslug--csv--slugjson">echo $BRAND_JSON | jq -r ‘.[] | [.domain.production,.slug] | @csv’ » slug.json</h1>

<p>“domain”,”brand”
“doamin1”,”brand1”</p>

<p>```</p>

<p>What I want is</p>

<p>``` text</p>

<p>“domain/url1”
“domain/url2”
“domain1/url3”</p>

<p>```</p>

<p>Basically, I want to join two files together by <code>brand</code> which I could have done that in SQL.</p>

<p>So my initial thought would be something like this</p>

<p>``` bash</p>

<p>BRAND_JSON=$(curl url.json)</p>

<p>while IFS= read -r line
do
  BRAND=$(echo $line | awk -F’,’ ‘{print $2}’ | sed “s/"//g”)
  URI=$(echo $line | awk -F’,’ ‘{print $1}’ | sed “s/"//g”)
  echo $BRAND
  DOMAIN=$(echo $BRAND_JSON | jq -r “.[] | select(.slug=="$BRAND") | .domain.production”)
  echo $DOMAIN
  echo $URI
  echo “https://$DOMAIN/$URI” » urls
done &lt; “brand_urls”</p>

<p>```</p>

<p>This is way too slow because I have about 900k of urls to filter and every url will do a O(n) search. Also, I created a lot of sub-shells</p>

<p>And I thought hmm if I can make that a constant operation using some kind of Map then it would be a lot faster. I immediately thought of Map or associative array but my shell doesn’t have associative just yet. People might wonder why can’t I just use python or node script to manipulate that. I could have but I just want a script to run in Jenkins and bash is available already. I don’t have to install any runtime.</p>

<h2 id="awk-to-the-rescue">Awk to the rescue</h2>
<p>Awk already has associate array!</p>

<p>Here’s my updated version of the code</p>

<p>``` bash</p>

<p>INPUTFILE=”slug.json”
DATAFILE=”brand_urls”
OUTFILE=”all_urls”</p>

<p>awk ‘BEGIN {
  # mapping brand.json
  while (getline &lt; “’“$INPUTFILE”’”)
  {
    split($0,ft,”,”);
    domain=ft[1];
    gsub(/”/, “”, domain);
    slug=ft[2];
    gsub(/”/, “”, slug);
    key=slug;
    data=domain;
    domainMap[key]=data;
  }
  close(“’“$INPUTFILE”’”);
  while (getline &lt; “’“$DATAFILE”’”)
    {
      split($0,ft,”,”);
      uri=ft[1];
      gsub(/”/, “”, uri);
      slug=ft[2];
      gsub(/”/, “”, slug);
      domain=domainMap[slug]; 
      printf “https://%s/%s\n”, domain, uri &gt; “’“$OUTFILE”’”;
    }
}’</p>

<p>```</p>

<p>I copied the script from <a href="https://magvar.wordpress.com/2011/05/18/awking-it-how-to-load-a-file-into-an-array-in-awk/">here</a></p>

]]></content>
  </entry>
  
</feed>
